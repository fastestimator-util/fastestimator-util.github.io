{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation Using Transformer\n",
    "\n",
    "[[Paper](https://arxiv.org/abs/1706.03762)] [[Notebook](https://github.com/fastestimator/fastestimator/blob/master/apphub/NLP/neural_machine_translation/transformer.ipynb)] [[TF Implementation](https://github.com/fastestimator/fastestimator/blob/master/apphub/NLP/neural_machine_translation/transformer_tf.py)] [[Torch Implementation](https://github.com/fastestimator/fastestimator/blob/master/apphub/NLP/neural_machine_translation/transformer_torch.py)]\n",
    "\n",
    "In this tutorial we will look at a sequence to sequence task: translating one language into another. The architecture used for the task is the famous `Transformer`.\n",
    "\n",
    "The general idea behind the transformer architecture is the `attention` mechanism that can perform a re-weighting of the features throughout the network. Another advantage brought by the transformer architecture is that it breaks the temporal dependency of the data, allowing more efficient parallelization of training. We will implement every detail of the transformer in this tutorial. Let's get started!\n",
    "\n",
    "First let's define some hyper-parameters that we will use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "data_dir = None\n",
    "epochs=20\n",
    "em_dim=128\n",
    "batch_size=64\n",
    "train_steps_per_epoch=None\n",
    "eval_steps_per_epoch=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "In this machine translation task, we will use the [TED translation dataset](https://github.com/neulab/word-embeddings-for-nmt). The dataset consists of 14 different translation tasks, such as Portuguese to English (`pt_to_en`), Russian to English (`ru_to_en`), and many others. In this tutorial, we will translate Portuguese to English. You can access this dataset through our dataset API - `tednmt`. Feel free to check the docstring of the API for other translation options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.dataset.data import tednmt\n",
    "\n",
    "train_ds, eval_ds, test_ds = tednmt.load_data(data_dir, translate_option=\"pt_to_en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the dataset is downloaded, let's check what the dataset looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example source language:\n",
      "entre todas as grandes privações com que nos debatemos hoje — pensamos em financeiras e económicas primeiro — aquela que mais me preocupa é a falta de diálogo político — a nossa capacidade de abordar conflitos modernos como eles são , de ir à raiz do que eles são e perceber os agentes-chave e lidar com eles .\n",
      "\n",
      "example target language:\n",
      "amongst all the troubling deficits we struggle with today — we think of financial and economic primarily — the ones that concern me most is the deficit of political dialogue — our ability to address modern conflicts as they are , to go to the source of what they 're all about and to understand the key players and to deal with them .\n"
     ]
    }
   ],
   "source": [
    "print(\"example source language:\")\n",
    "print(train_ds[0][\"source\"])\n",
    "print(\"\")\n",
    "print(\"example target language:\")\n",
    "print(train_ds[0][\"target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the languages\n",
    "\n",
    "Since the text by itself cannot be recognized by computers, we need to perform a series of transformations to the text. Here are the steps:\n",
    "1. Split the sentence into words or sub-words. For example, \"I love apple\" can be split into [\"I\", \"love\", \"apple\"]. Sometimes in order to represent more words, a word is further reduced into sub-words. For example, `tokenization` can be split into `token` and `_ization`. As a result, a word like \"civilization\" doesn't require extra space when both `civil` and `_ization` are already in the dictionary.\n",
    "2. Map the tokens into a discrete index according to the dictionary. In this task, we are loading a pretrained tokenizer with a built-in dictionary already.\n",
    "3. Add a [start] and [end] token around every index. This is mainly to help the network identify the beginning and end of each sentence.\n",
    "4. When creating a batch of multiple sentences, pad the shorter sentences with 0 so that each sentence in the batch has the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastestimator as fe\n",
    "from transformers import BertTokenizer\n",
    "from fastestimator.op.numpyop import Batch, NumpyOp\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Encode(NumpyOp):\n",
    "    def __init__(self, tokenizer, inputs, outputs, mode=None):\n",
    "        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def forward(self, data, state):\n",
    "        return np.array(self.tokenizer.encode(data))\n",
    "\n",
    "\n",
    "pt_tokenizer = BertTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
    "en_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "pipeline = fe.Pipeline(\n",
    "    train_data=train_ds,\n",
    "    eval_data=eval_ds,\n",
    "    test_data=test_ds,\n",
    "    ops=[\n",
    "        Encode(inputs=\"source\", outputs=\"source\", tokenizer=pt_tokenizer),\n",
    "        Encode(inputs=\"target\", outputs=\"target\", tokenizer=en_tokenizer),\n",
    "        Batch(batch_size=batch_size, pad_value=0)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, `tokenizer.encode` will take the sentence and execute the step 1 - 3. The padding step is done by providing `pad_value=0` in the `Batch` Op. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source after processing:\n",
      "tensor([[  101,   420,  1485,  ...,  1061,   119,   102],\n",
      "        [  101,   538,   179,  ...,     0,     0,     0],\n",
      "        [  101,   122, 21174,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,   607,   230,  ...,     0,     0,     0],\n",
      "        [  101,   123, 10186,  ...,     0,     0,     0],\n",
      "        [  101, 11865,  3072,  ...,     0,     0,     0]])\n",
      "source batch shape:\n",
      "torch.Size([64, 72])\n",
      "---------------------------------------------------\n",
      "target after processing:\n",
      "tensor([[ 101, 5921, 2035,  ..., 2068, 1012,  102],\n",
      "        [ 101, 2057, 2040,  ...,    0,    0,    0],\n",
      "        [ 101, 1998, 1045,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 2045, 1005,  ...,    0,    0,    0],\n",
      "        [ 101, 1996, 5424,  ...,    0,    0,    0],\n",
      "        [ 101, 2009, 2097,  ...,    0,    0,    0]])\n",
      "target batch shape:\n",
      "torch.Size([64, 70])\n"
     ]
    }
   ],
   "source": [
    "data = pipeline.get_results()\n",
    "print(\"source after processing:\")\n",
    "print(data[\"source\"])\n",
    "print(\"source batch shape:\")\n",
    "print(data[\"source\"].shape)\n",
    "print(\"---------------------------------------------------\")\n",
    "print(\"target after processing:\")\n",
    "print(data[\"target\"])\n",
    "print(\"target batch shape:\")\n",
    "print(data[\"target\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Architecture\n",
    "## Attention Unit\n",
    "The basic form of the attention unit is defined in `scaled_dot_product_attention`. Given a set of queries(Q), keys(K), and values(V), it first performs the matrix multiplication of Q and K. The output of this multiplication gives the matching score between various elements of Q and K. Then all the weights are normalized across the Keys dimension. Finally, the normalized score will be multiplied by the V to get the final result.  The intuition behind the attention unit is essentially a dictionary look-up with interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "    num_heads, inp_length = tf.shape(scaled_attention_logits)[1], tf.shape(scaled_attention_logits)[2]\n",
    "    num_heads_mask, inp_length_mask = tf.shape(mask)[1], tf.shape(mask)[2]\n",
    "    # This manual tiling is to fix a auto-broadcasting issue with tensorflow\n",
    "    scaled_attention_logits += tf.tile(mask * -1e9, [1, num_heads // num_heads_mask, inp_length // inp_length_mask, 1])\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "    return output\n",
    "\n",
    "\n",
    "def point_wise_feed_forward_network(em_dim, dff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "        tf.keras.layers.Dense(em_dim)  # (batch_size, seq_len, em_dim)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-head Attention\n",
    "\n",
    "There are two drawbacks of the attention unit above:\n",
    "1. The complexity of matrix multiplication is O(N^3), when batch size or embedding dimension increases, the computation will not scale well. \n",
    "2. A single attention head is limited in expressing local correlation between two words, because it calculates correlation by normalizing all embeddings dimensions. Sometimes this overall normalization will remove interesting local patterns.  A good analogy is to consider a single attention unit as globally averaging a signal whereas a moving average is preferred to preserve certain information.\n",
    "\n",
    "Multi-head attention is used to overcome the issues above. It breaks the embedding dimension into multiple heads. As a result, each head's embedding dimension is divided by the number of heads, reducing the computation complexity. Moreover, each head only takes a fraction of the embedding and can be viewed as a specialized expert for a specific context.  The final results can be combined using another dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "class MultiHeadAttention(layers.Layer):\n",
    "    def __init__(self, em_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert em_dim % num_heads == 0, \"model dimension must be multiply of number of heads\"\n",
    "        self.num_heads = num_heads\n",
    "        self.em_dim = em_dim\n",
    "        self.depth = em_dim // self.num_heads\n",
    "        self.wq = layers.Dense(em_dim)\n",
    "        self.wk = layers.Dense(em_dim)\n",
    "        self.wv = layers.Dense(em_dim)\n",
    "        self.dense = layers.Dense(em_dim)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])  # B, num_heads, seq_len, depth\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        q = self.wq(q)  # B, seq_len, em_dim\n",
    "        k = self.wk(k)  # B, seq_len, em_dim\n",
    "        v = self.wv(v)  # B, seq_len, em_dim\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "        scaled_attention = scaled_dot_product_attention(q, k, v, mask)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  #B, seq_len, num_heads, depth\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.em_dim))  # B, seq_len, em_dim\n",
    "        output = self.dense(concat_attention)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder and Decoder layer\n",
    "\n",
    "Both the encoder and decoder layers will go through multi-head attention. The decoder layer will use another multi-attention module to connect the bridge between encoder outputs and targets. Specifically, in the decoders second multi-head attention module, encoded output is used as both values and keys whereas the target embedding is used as a query to \"look up\" encoder information. In the end, there is a feed-forward neural network to transform the looked-up value into something useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(layers.Layer):\n",
    "    def __init__(self, em_dim, num_heads, dff, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(em_dim, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(em_dim, dff)\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        attn_output = self.mha(x, x, x, mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        return out2\n",
    "\n",
    "\n",
    "class DecoderLayer(layers.Layer):\n",
    "    def __init__(self, em_dim, num_heads, diff, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.mha1 = MultiHeadAttention(em_dim, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(em_dim, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(em_dim, diff)\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "        self.dropout3 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_out, training, decode_mask, padding_mask):\n",
    "        attn1 = self.mha1(x, x, x, decode_mask)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "        attn2 = self.mha2(enc_out, enc_out, out1, padding_mask)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)\n",
    "        return out3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting Everything Together\n",
    "\n",
    "A transformer consists of an Encoder and Decoder, which in turn consist of multiple stacked encoder/decoder layers. One interesting property of transformers is that they do not have an intrinsic awareness of the position dimension. Therefore, a position encoding is usually done to the embedding matrix to add position context to the embedding. A nice tutorial about positional encoding can be found [here](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, em_dim):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(em_dim))\n",
    "    return pos * angle_rates\n",
    "\n",
    "\n",
    "def positional_encoding(position, em_dim):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis], np.arange(em_dim)[np.newaxis, :], em_dim)\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "\n",
    "class Encoder(layers.Layer):\n",
    "    def __init__(self, num_layers, em_dim, num_heads, dff, input_vocab, max_pos_enc, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.em_dim = em_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = layers.Embedding(input_vocab, em_dim)\n",
    "        self.pos_encoding = positional_encoding(max_pos_enc, self.em_dim)\n",
    "        self.enc_layers = [EncoderLayer(em_dim, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, mask, training=None):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.em_dim, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        x = self.dropout(x, training=training)\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(layers.Layer):\n",
    "    def __init__(self, num_layers, em_dim, num_heads, dff, target_vocab, max_pos_enc, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.em_dim = em_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = layers.Embedding(target_vocab, em_dim)\n",
    "        self.pos_encoding = positional_encoding(max_pos_enc, em_dim)\n",
    "        self.dec_layers = [DecoderLayer(em_dim, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, decode_mask, padding_mask, training=None):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.em_dim, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        x = self.dropout(x, training=training)\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.dec_layers[i](x, enc_output, training, decode_mask, padding_mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "def transformer(num_layers, em_dim, num_heads, dff, input_vocab, target_vocab, max_pos_enc, max_pos_dec, rate=0.1):\n",
    "    inputs = layers.Input(shape=(None, ))\n",
    "    targets = layers.Input(shape=(None, ))\n",
    "    encode_mask = layers.Input(shape=(None, None, None))\n",
    "    decode_mask = layers.Input(shape=(None, None, None))\n",
    "    x = Encoder(num_layers, em_dim, num_heads, dff, input_vocab, max_pos_enc, rate=rate)(inputs, encode_mask)\n",
    "    x = Decoder(num_layers, em_dim, num_heads, dff, target_vocab, max_pos_dec, rate=rate)(targets,\n",
    "                                                                                          x,\n",
    "                                                                                          decode_mask,\n",
    "                                                                                          encode_mask)\n",
    "    x = layers.Dense(target_vocab)(x)\n",
    "    model = tf.keras.Model(inputs=[inputs, targets, encode_mask, decode_mask], outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fe.build(\n",
    "    model_fn=lambda: transformer(num_layers=4,\n",
    "                                 em_dim=em_dim,\n",
    "                                 num_heads=8,\n",
    "                                 dff=512,\n",
    "                                 input_vocab=pt_tokenizer.vocab_size,\n",
    "                                 target_vocab=en_tokenizer.vocab_size,\n",
    "                                 max_pos_enc=1000,\n",
    "                                 max_pos_dec=1000),\n",
    "    optimizer_fn=\"adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Operations\n",
    "\n",
    "Now that we have defined the transformer architecture, another thing that is worth mentioning is the mask. A mask is a boolean array that we created to tell the network to **ignore** certain words within the sentence. For example, to tell the network to ignore the words we padded, a padding mask is used. In order to not give away the answer when processing the word before it, a mask is also needed.\n",
    "\n",
    "The loss function of transformer is simply a masked cross entropy loss, as it will only consider predictions that are not masked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.op.tensorop import TensorOp\n",
    "from fastestimator.op.tensorop.loss import LossOp\n",
    "from fastestimator.op.tensorop.model import ModelOp, UpdateOp\n",
    "\n",
    "class CreateMasks(TensorOp):\n",
    "    def forward(self, data, state):\n",
    "        inp, tar = data\n",
    "        encode_mask = self.create_padding_mask(inp)\n",
    "        dec_look_ahead_mask = self.create_look_ahead_mask(tf.shape(tar)[1])\n",
    "        dec_target_padding_mask = self.create_padding_mask(tar)\n",
    "        decode_mask = tf.maximum(dec_target_padding_mask, dec_look_ahead_mask)\n",
    "        return encode_mask, decode_mask\n",
    "\n",
    "    @staticmethod\n",
    "    def create_padding_mask(seq):\n",
    "        seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "        return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_look_ahead_mask(size):\n",
    "        mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "        return mask  # (seq_len, seq_len)\n",
    "\n",
    "\n",
    "class ShiftData(TensorOp):\n",
    "    def forward(self, data, state):\n",
    "        target = data\n",
    "        return target[:, :-1], target[:, 1:]\n",
    "\n",
    "\n",
    "class MaskedCrossEntropy(LossOp):\n",
    "    def __init__(self, inputs, outputs, mode=None):\n",
    "        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n",
    "        self.loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "    def forward(self, data, state):\n",
    "        y_pred, y_true = data\n",
    "        mask = tf.cast(tf.math.logical_not(tf.math.equal(y_true, 0)), tf.float32)\n",
    "        loss = self.loss_fn(y_true, y_pred) * mask\n",
    "        loss = tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "        return loss\n",
    "\n",
    "network = fe.Network(ops=[\n",
    "    ShiftData(inputs=\"target\", outputs=(\"target_inp\", \"target_real\")),\n",
    "    CreateMasks(inputs=(\"source\", \"target_inp\"), outputs=(\"encode_mask\", \"decode_mask\")),\n",
    "    ModelOp(model=model, inputs=(\"source\", \"target_inp\", \"encode_mask\", \"decode_mask\"), outputs=\"pred\"),\n",
    "    MaskedCrossEntropy(inputs=(\"pred\", \"target_real\"), outputs=\"ce\"),\n",
    "    UpdateOp(model=model, loss_name=\"ce\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics and Learning Rate Scheduling\n",
    "\n",
    "The metric used to evaluate the model is a masked accuracy, which is simply accuracy with unmasked predictions and ground truths. The learning rate scheduler uses warm-up followed by exponential decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "from fastestimator.trace.adapt import LRScheduler\n",
    "from fastestimator.trace.io import BestModelSaver\n",
    "from fastestimator.trace.metric.bleu_score import BleuScore\n",
    "from fastestimator.trace.trace import Trace\n",
    "\n",
    "model_dir=tempfile.mkdtemp()\n",
    "\n",
    "def lr_fn(step, em_dim, warmupstep=4000):\n",
    "    lr = em_dim**-0.5 * min(step**-0.5, step * warmupstep**-1.5)\n",
    "    return lr\n",
    "\n",
    "class MaskedAccuracy(Trace):\n",
    "    def on_epoch_begin(self, data):\n",
    "        self.correct = 0\n",
    "        self.total = 0\n",
    "\n",
    "    def on_batch_end(self, data):\n",
    "        y_pred, y_true = data[\"pred\"].numpy(), data[\"target_real\"].numpy()\n",
    "        mask = np.logical_not(y_true == 0)\n",
    "        matches = np.logical_and(y_true == np.argmax(y_pred, axis=2), mask)\n",
    "        self.correct += np.sum(matches)\n",
    "        self.total += np.sum(mask)\n",
    "\n",
    "    def on_epoch_end(self, data):\n",
    "        data.write_with_log(self.outputs[0], self.correct / self.total)\n",
    "\n",
    "\n",
    "traces = [\n",
    "    MaskedAccuracy(inputs=(\"pred\", \"target_real\"), outputs=\"masked_acc\", mode=\"!train\"),\n",
    "    BleuScore(true_key=\"target_real\", pred_key =\"pred\", output_name=\"bleu_score\", n_gram=2, mode=\"!train\"),\n",
    "    BestModelSaver(model=model, save_dir=model_dir, metric=\"masked_acc\", save_best_mode=\"max\"),\n",
    "    LRScheduler(model=model, lr_fn=lambda step: lr_fn(step, em_dim))]\n",
    "\n",
    "estimator = fe.Estimator(pipeline=pipeline,\n",
    "                         network=network,\n",
    "                         traces=traces,\n",
    "                         epochs=epochs,\n",
    "                         train_steps_per_epoch=train_steps_per_epoch,\n",
    "                         eval_steps_per_epoch=eval_steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start the training\n",
    "\n",
    "The training will take around 30 minutes on a single V100 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's translate something!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_to_words(sample, tokenizer):\n",
    "    words = tokenizer.decode(sample)\n",
    "    if '[CLS]' in words:\n",
    "        words = words[words.index('[CLS]')+5:]\n",
    "    if '[SEP]' in words:\n",
    "        words = words[:words.index('[SEP]')]\n",
    "    return words\n",
    "\n",
    "sample_test_data = pipeline.get_results(mode=\"test\")\n",
    "sample_test_data = network.transform(data=sample_test_data, mode=\"test\")\n",
    "source = sample_test_data[\"source\"].numpy()\n",
    "predicted = sample_test_data[\"pred\"].numpy()\n",
    "predicted = np.argmax(predicted, axis=-1)\n",
    "grouth_truth = sample_test_data[\"target_real\"].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Language: \n",
      " muito obrigado. \n",
      "\n",
      "Translation Ground Truth: \n",
      "thank you very much. \n",
      "\n",
      "Machine Translation: \n",
      "thank you very much. \n"
     ]
    }
   ],
   "source": [
    "index = np.random.randint(0, source.shape[0])\n",
    "sample_source, sample_predicted, sample_groud_truth = source[index], predicted[index], grouth_truth[index]\n",
    "print(\"Source Language: \")\n",
    "print(token_to_words(sample_source, pt_tokenizer))\n",
    "print(\"\")\n",
    "print(\"Translation Ground Truth: \")\n",
    "print(token_to_words(sample_groud_truth, en_tokenizer))\n",
    "print(\"\")\n",
    "print(\"Machine Translation: \")\n",
    "print(token_to_words(sample_predicted, en_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are welcome."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
