{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Prediction in IMDB Reviews using an LSTM\n",
    "\n",
    "[[Notebook](https://github.com/fastestimator/fastestimator/blob/master/apphub/NLP/imdb/imdb.ipynb)] [[TF Implementation](https://github.com/fastestimator/fastestimator/blob/master/apphub/NLP/imdb/imdb_tf.py)] [[Torch Implementation](https://github.com/fastestimator/fastestimator/blob/master/apphub/NLP/imdb/imdb_torch.py)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import fastestimator as fe\n",
    "from fastestimator.dataset.data import imdb_review\n",
    "from fastestimator.op.numpyop.univariate.reshape import Reshape\n",
    "from fastestimator.op.tensorop.loss import CrossEntropy\n",
    "from fastestimator.op.tensorop.model import ModelOp, UpdateOp\n",
    "from fastestimator.trace.io import BestModelSaver\n",
    "from fastestimator.trace.metric import Accuracy\n",
    "from fastestimator.backend import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "MAX_WORDS = 10000\n",
    "MAX_LEN = 500\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "train_steps_per_epoch = None\n",
    "eval_steps_per_epoch = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Building components</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Prepare training & evaluation data and define a `Pipeline`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are loading the dataset from tf.keras.datasets.imdb which contains movie reviews and sentiment scores. All the words have been replaced with the integers that specifies the popularity of the word in corpus. To ensure all the sequences are of same length we need to pad the input sequences before defining the `Pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, eval_data = imdb_review.load_data(MAX_LEN, MAX_WORDS)\n",
    "pipeline = fe.Pipeline(train_data=train_data,\n",
    "                       eval_data=eval_data,\n",
    "                       batch_size=batch_size,\n",
    "                       ops=Reshape(1, inputs=\"y\", outputs=\"y\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create a `model` and FastEstimator `Network`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to define the neural network architecture, and then pass the definition, associated model name, and optimizer into fe.build:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewSentiment(nn.Module):\n",
    "    def __init__(self, embedding_size=64, hidden_units=64):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(MAX_WORDS, embedding_size)\n",
    "        self.conv1d = nn.Conv1d(in_channels=64, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.maxpool1d = nn.MaxPool1d(kernel_size=4)\n",
    "        self.lstm = nn.LSTM(input_size=125, hidden_size=hidden_units, num_layers=1)\n",
    "        self.fc1 = nn.Linear(in_features=hidden_units, out_features=250)\n",
    "        self.fc2 = nn.Linear(in_features=250, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute((0, 2, 1))\n",
    "        x = self.conv1d(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.maxpool1d(x)\n",
    "        output, _ = self.lstm(x)\n",
    "        x = output[:, -1]  # sequence output of only last timestamp\n",
    "        x = torch.tanh(x)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Network` is the object that defines the whole training graph, including models, loss functions, optimizers etc. A `Network` can have several different models and loss functions (ex. GANs). `fe.Network` takes a series of operators, in this case just the basic `ModelOp`, loss op, and `UpdateOp` will suffice. It should be noted that \"y_pred\" is the key in the data dictionary which will store the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fe.build(model_fn=lambda: ReviewSentiment(), optimizer_fn=\"adam\")\n",
    "network = fe.Network(ops=[\n",
    "    ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n",
    "    CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"loss\"),\n",
    "    UpdateOp(model=model, loss_name=\"loss\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Prepare `Estimator` and configure the training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Estimator` is the API that wraps the `Pipeline`, `Network` and other training metadata together. `Estimator` also contains `Traces`, which are similar to the callbacks of Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the training loop, we want to measure the validation loss and save the model that has the minimum loss. `BestModelSaver` is a convenient `Trace` to achieve this. Let's also measure accuracy over time using another `Trace`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = tempfile.mkdtemp()\n",
    "traces = [Accuracy(true_key=\"y\", pred_key=\"y_pred\"), BestModelSaver(model=model, save_dir=model_dir)]\n",
    "estimator = fe.Estimator(network=network,\n",
    "                         pipeline=pipeline,\n",
    "                         epochs=epochs,\n",
    "                         traces=traces,\n",
    "                         train_steps_per_epoch=train_steps_per_epoch,\n",
    "                         eval_steps_per_epoch=eval_steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ______           __  ______     __  _                 __            \n",
      "   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n",
      "  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n",
      " / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n",
      "/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n",
      "                                                                        \n",
      "\n",
      "FastEstimator-Start: step: 1; logging_interval: 100; num_device: 0;\n",
      "FastEstimator-Train: step: 1; loss: 0.6982045;\n",
      "FastEstimator-Train: step: 100; loss: 0.69076145; steps/sec: 4.55;\n",
      "FastEstimator-Train: step: 200; loss: 0.6970146; steps/sec: 5.49;\n",
      "FastEstimator-Train: step: 300; loss: 0.67406845; steps/sec: 5.6;\n",
      "FastEstimator-Train: step: 358; epoch: 1; epoch_time: 69.22 sec;\n",
      "FastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpds6dz9wa/model_best_loss.pt\n",
      "FastEstimator-Eval: step: 358; epoch: 1; accuracy: 0.6826793843485801; loss: 0.59441286; min_loss: 0.59441286; since_best_loss: 0;\n",
      "FastEstimator-Train: step: 400; loss: 0.579373; steps/sec: 5.39;\n",
      "FastEstimator-Train: step: 500; loss: 0.5601772; steps/sec: 4.79;\n",
      "FastEstimator-Train: step: 600; loss: 0.3669433; steps/sec: 5.2;\n",
      "FastEstimator-Train: step: 700; loss: 0.5050458; steps/sec: 4.86;\n",
      "FastEstimator-Train: step: 716; epoch: 2; epoch_time: 71.36 sec;\n",
      "FastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpds6dz9wa/model_best_loss.pt\n",
      "FastEstimator-Eval: step: 716; epoch: 2; accuracy: 0.7672230652503793; loss: 0.48858097; min_loss: 0.48858097; since_best_loss: 0;\n",
      "FastEstimator-Train: step: 800; loss: 0.43962425; steps/sec: 5.57;\n",
      "FastEstimator-Train: step: 900; loss: 0.33729357; steps/sec: 5.71;\n",
      "FastEstimator-Train: step: 1000; loss: 0.31596264; steps/sec: 5.23;\n",
      "FastEstimator-Train: step: 1074; epoch: 3; epoch_time: 77.79 sec;\n",
      "FastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpds6dz9wa/model_best_loss.pt\n",
      "FastEstimator-Eval: step: 1074; epoch: 3; accuracy: 0.8103186646433991; loss: 0.4192897; min_loss: 0.4192897; since_best_loss: 0;\n",
      "FastEstimator-Train: step: 1100; loss: 0.33041656; steps/sec: 3.22;\n",
      "FastEstimator-Train: step: 1200; loss: 0.41677344; steps/sec: 5.75;\n",
      "FastEstimator-Train: step: 1300; loss: 0.43493804; steps/sec: 5.68;\n",
      "FastEstimator-Train: step: 1400; loss: 0.26938343; steps/sec: 5.34;\n",
      "FastEstimator-Train: step: 1432; epoch: 4; epoch_time: 64.02 sec;\n",
      "FastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpds6dz9wa/model_best_loss.pt\n",
      "FastEstimator-Eval: step: 1432; epoch: 4; accuracy: 0.823845653587687; loss: 0.3995199; min_loss: 0.3995199; since_best_loss: 0;\n",
      "FastEstimator-Train: step: 1500; loss: 0.323763; steps/sec: 5.76;\n",
      "FastEstimator-Train: step: 1600; loss: 0.21561582; steps/sec: 5.84;\n",
      "FastEstimator-Train: step: 1700; loss: 0.20746922; steps/sec: 5.59;\n",
      "FastEstimator-Train: step: 1790; epoch: 5; epoch_time: 63.49 sec;\n",
      "FastEstimator-Eval: step: 1790; epoch: 5; accuracy: 0.8291784088445697; loss: 0.4008124; min_loss: 0.3995199; since_best_loss: 1;\n",
      "FastEstimator-Train: step: 1800; loss: 0.2219275; steps/sec: 5.12;\n",
      "FastEstimator-Train: step: 1900; loss: 0.2188505; steps/sec: 5.11;\n",
      "FastEstimator-Train: step: 2000; loss: 0.14373234; steps/sec: 5.53;\n",
      "FastEstimator-Train: step: 2100; loss: 0.20883155; steps/sec: 1.96;\n",
      "FastEstimator-Train: step: 2148; epoch: 6; epoch_time: 100.15 sec;\n",
      "FastEstimator-Eval: step: 2148; epoch: 6; accuracy: 0.8313461955343594; loss: 0.41437832; min_loss: 0.3995199; since_best_loss: 2;\n",
      "FastEstimator-Train: step: 2200; loss: 0.20082837; steps/sec: 5.64;\n",
      "FastEstimator-Train: step: 2300; loss: 0.22870378; steps/sec: 5.65;\n",
      "FastEstimator-Train: step: 2400; loss: 0.28569937; steps/sec: 5.7;\n",
      "FastEstimator-Train: step: 2500; loss: 0.16878708; steps/sec: 5.69;\n",
      "FastEstimator-Train: step: 2506; epoch: 7; epoch_time: 63.07 sec;\n",
      "FastEstimator-Eval: step: 2506; epoch: 7; accuracy: 0.8314762627357468; loss: 0.42922923; min_loss: 0.3995199; since_best_loss: 3;\n",
      "FastEstimator-Train: step: 2600; loss: 0.20338291; steps/sec: 5.77;\n",
      "FastEstimator-Train: step: 2700; loss: 0.17639604; steps/sec: 5.68;\n",
      "FastEstimator-Train: step: 2800; loss: 0.12155069; steps/sec: 5.7;\n",
      "FastEstimator-Train: step: 2864; epoch: 8; epoch_time: 62.75 sec;\n",
      "FastEstimator-Eval: step: 2864; epoch: 8; accuracy: 0.8294818989811402; loss: 0.46396694; min_loss: 0.3995199; since_best_loss: 4;\n",
      "FastEstimator-Train: step: 2900; loss: 0.20103803; steps/sec: 5.34;\n",
      "FastEstimator-Train: step: 3000; loss: 0.10518805; steps/sec: 5.71;\n",
      "FastEstimator-Train: step: 3100; loss: 0.10425654; steps/sec: 5.64;\n",
      "FastEstimator-Train: step: 3200; loss: 0.13740686; steps/sec: 5.5;\n",
      "FastEstimator-Train: step: 3222; epoch: 9; epoch_time: 64.67 sec;\n",
      "FastEstimator-Eval: step: 3222; epoch: 9; accuracy: 0.8254498157381314; loss: 0.5149529; min_loss: 0.3995199; since_best_loss: 5;\n",
      "FastEstimator-Train: step: 3300; loss: 0.080922514; steps/sec: 5.77;\n",
      "FastEstimator-Train: step: 3400; loss: 0.088989146; steps/sec: 5.41;\n",
      "FastEstimator-Train: step: 3500; loss: 0.1620798; steps/sec: 5.3;\n",
      "FastEstimator-Train: step: 3580; epoch: 10; epoch_time: 64.87 sec;\n",
      "FastEstimator-Eval: step: 3580; epoch: 10; accuracy: 0.8214177324951225; loss: 0.5555562; min_loss: 0.3995199; since_best_loss: 6;\n",
      "FastEstimator-Finish: step: 3580; model_lr: 0.001; total_time: 1124.45 sec;\n"
     ]
    }
   ],
   "source": [
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Inferencing</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For inferencing, first we have to load the trained model weights. We previously saved model weights corresponding to our minimum loss, and now we will load the weights using `load_model()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_best_loss.pt'\n",
    "model_path = os.path.join(model_dir, model_name)\n",
    "load_model(model, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get some random sequence and compare the prediction with the ground truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth is:  1\n"
     ]
    }
   ],
   "source": [
    "selected_idx = np.random.randint(10000)\n",
    "print(\"Ground truth is: \",eval_data[selected_idx]['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data dictionary for the inference. The `Transform()` function in Pipeline and Network applies all the operations on the given data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_data = {\"x\":eval_data[selected_idx]['x'], \"y\":eval_data[selected_idx]['y']}\n",
    "data = pipeline.transform(infer_data, mode=\"infer\")\n",
    "data = network.transform(data, mode=\"infer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, print the inferencing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for the input sequence:  0.91389465\n"
     ]
    }
   ],
   "source": [
    "print(\"Prediction for the input sequence: \", np.array(data[\"y_pred\"])[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using your own dataset\n",
    "This example can be used for any custom dataset that requires a `sequence-to-vector` task. The `Pipeline` in this code example assumes a tokenized sentence (every word represented by an index) in each sample with a fixed length (0-padded). \n",
    "\n",
    "If you have a tokenized dataset already, you can create a `Dataset` class that produces an output similar to the code example. If your dataset is not yet tokenized (aka in words), you can use a `Tokenize` Operator similar to [This example](https://github.com/fastestimator/fastestimator/blob/master/apphub/NLP/named_entity_recognition/bert.ipynb). "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
