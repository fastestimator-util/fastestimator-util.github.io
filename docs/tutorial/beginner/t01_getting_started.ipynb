{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: Getting Started\n",
    "\n",
    "## Overview\n",
    "Welcome to FastEstimator! In this tutorial we are going to cover:\n",
    "* [The three main APIs of FastEstimator: `Pipeline`, `Network`, `Estimator`](#t01ThreeMain)\n",
    "* [An image classification example](#t01ImageClassification)\n",
    "    * [Pipeline](#t01Pipeline)\n",
    "    * [Network](#t01Network)\n",
    "    * [Estimator](#t01Estimator)\n",
    "    * [Training](#t01Training)\n",
    "    * [Inferencing](#t01Inferencing)\n",
    "* [Related Apphub Examples](#t01Apphub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='t01ThreeMain'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three main APIs\n",
    "All deep learning training workï¬‚ows involve the following three essential components, each mapping to a critical API in FastEstimator.\n",
    "\n",
    "* **Data pipeline**: extracts data from disk/RAM, performs transformations. ->  `fe.Pipeline`\n",
    "\n",
    "\n",
    "* **Network**: performs trainable and differentiable operations. ->  `fe.Network`\n",
    "\n",
    "\n",
    "* **Training loop**: combines the data pipeline and network in an iterative process. ->  `fe.Estimator`\n",
    "\n",
    "<BR>\n",
    "<BR>\n",
    "Any deep learning task can be constructed by following the 3 main steps:\n",
    "<BR>\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/fastestimator-util/fastestimator-misc/blob/master/resource/pictures/tutorial/t01_api.png?raw=true\" alt=\"drawing\" width=\"700\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='t01ImageClassification'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Classification Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='t01Pipeline'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Pipeline\n",
    "We use FastEstimator dataset API to load the MNIST dataset. Please check out [Tutorial 2](./t02_dataset.ipynb) for more details about the dataset API. In this case our data preprocessing involves: \n",
    "1. Expand image dimension from (28,28) to (28, 28, 1) for convenience during convolution operations.\n",
    "2. Rescale pixel values from [0, 255] to [0, 1].\n",
    "\n",
    "Please check out [Tutorial 3](./t03_operator.ipynb) for details about `Operator` and [Tutorial 4](./t04_pipeline.ipynb) for `Pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastestimator as fe\n",
    "from fastestimator.dataset.data import mnist\n",
    "from fastestimator.op.numpyop.univariate import ExpandDims, Minmax\n",
    "\n",
    "train_data, eval_data = mnist.load_data()\n",
    "\n",
    "pipeline = fe.Pipeline(train_data=train_data,\n",
    "                       eval_data=eval_data,\n",
    "                       batch_size=32,\n",
    "                       ops=[ExpandDims(inputs=\"x\", outputs=\"x\"), Minmax(inputs=\"x\", outputs=\"x\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='t01Network'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Network\n",
    "\n",
    "The model definition can be either from `tf.keras.Model` or `torch.nn.Module`, for more info about network definitions, check out [Tutorial 5](./t05_model.ipynb). The differentiable operations during training are listed as follows:\n",
    "\n",
    "1. Feed the preprocessed images to the network and get prediction scores.\n",
    "2. Calculate `CrossEntropy` (loss) between prediction scores and ground truth.\n",
    "3. Update the model by minimizing `CrossEntropy`.\n",
    "\n",
    "For more info about `Network` and its operators, check out [Tutorial 6](./t06_network.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.architecture.tensorflow import LeNet\n",
    "# from fastestimator.architecture.pytorch import LeNet  # One can also use a pytorch model\n",
    "\n",
    "from fastestimator.op.tensorop.loss import CrossEntropy\n",
    "from fastestimator.op.tensorop.model import ModelOp, UpdateOp\n",
    "\n",
    "model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\")\n",
    "\n",
    "network = fe.Network(ops=[\n",
    "        ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n",
    "        CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),\n",
    "        UpdateOp(model=model, loss_name=\"ce\") \n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='t01Estimator'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Estimator\n",
    "We define the `Estimator` to connect the `Network` to the `Pipeline`, and compute accuracy as a validation metric. Please see [Tutorial 7](./t07_estimator.ipynb) for more about `Estimator` and `Traces`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.trace.metric import Accuracy\n",
    "from fastestimator.trace.io import BestModelSaver\n",
    "import tempfile\n",
    "\n",
    "traces = [Accuracy(true_key=\"y\", pred_key=\"y_pred\"),\n",
    "          BestModelSaver(model=model, save_dir=tempfile.mkdtemp(), metric=\"accuracy\", save_best_mode=\"max\")]\n",
    "\n",
    "estimator = fe.Estimator(pipeline=pipeline,\n",
    "                         network=network,\n",
    "                         epochs=2,\n",
    "                         traces=traces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='t01Training'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ______           __  ______     __  _                 __            \n",
      "   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n",
      "  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n",
      " / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n",
      "/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n",
      "                                                                        \n",
      "\n",
      "FastEstimator-Start: step: 1; logging_interval: 100; num_device: 0;\n",
      "FastEstimator-Train: step: 1; ce: 2.308363;\n",
      "FastEstimator-Train: step: 100; ce: 0.32775605; steps/sec: 76.05;\n",
      "FastEstimator-Train: step: 200; ce: 0.09257892; steps/sec: 73.5;\n",
      "FastEstimator-Train: step: 300; ce: 0.34083164; steps/sec: 76.4;\n",
      "FastEstimator-Train: step: 400; ce: 0.1040692; steps/sec: 74.74;\n",
      "FastEstimator-Train: step: 500; ce: 0.021515703; steps/sec: 77.2;\n",
      "FastEstimator-Train: step: 600; ce: 0.06389343; steps/sec: 76.94;\n",
      "FastEstimator-Train: step: 700; ce: 0.081933156; steps/sec: 76.56;\n",
      "FastEstimator-Train: step: 800; ce: 0.04755495; steps/sec: 75.82;\n",
      "FastEstimator-Train: step: 900; ce: 0.09036083; steps/sec: 76.68;\n",
      "FastEstimator-Train: step: 1000; ce: 0.076977566; steps/sec: 73.93;\n",
      "FastEstimator-Train: step: 1100; ce: 0.006916199; steps/sec: 74.92;\n",
      "FastEstimator-Train: step: 1200; ce: 0.116034895; steps/sec: 72.66;\n",
      "FastEstimator-Train: step: 1300; ce: 0.0036065953; steps/sec: 73.45;\n",
      "FastEstimator-Train: step: 1400; ce: 0.1516164; steps/sec: 73.41;\n",
      "FastEstimator-Train: step: 1500; ce: 0.066313066; steps/sec: 73.17;\n",
      "FastEstimator-Train: step: 1600; ce: 0.1330988; steps/sec: 71.1;\n",
      "FastEstimator-Train: step: 1700; ce: 0.08547261; steps/sec: 70.95;\n",
      "FastEstimator-Train: step: 1800; ce: 0.024575546; steps/sec: 72.46;\n",
      "FastEstimator-Train: step: 1875; epoch: 1; epoch_time: 26.76 sec;\n",
      "Eval Progress: 1/312;\n",
      "Eval Progress: 104/312; steps/sec: 222.3;\n",
      "Eval Progress: 208/312; steps/sec: 213.34;\n",
      "Eval Progress: 312/312; steps/sec: 210.13;\n",
      "FastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpz8b4p4k4/model_best_accuracy.h5\n",
      "FastEstimator-Eval: step: 1875; epoch: 1; accuracy: 0.9874; ce: 0.042314235; max_accuracy: 0.9874; since_best_accuracy: 0;\n",
      "FastEstimator-Train: step: 1900; ce: 0.018547835; steps/sec: 51.8;\n",
      "FastEstimator-Train: step: 2000; ce: 0.02083261; steps/sec: 72.46;\n",
      "FastEstimator-Train: step: 2100; ce: 0.0013426461; steps/sec: 72.69;\n",
      "FastEstimator-Train: step: 2200; ce: 0.03557183; steps/sec: 72.89;\n",
      "FastEstimator-Train: step: 2300; ce: 0.016057294; steps/sec: 71.61;\n",
      "FastEstimator-Train: step: 2400; ce: 0.019565197; steps/sec: 72.94;\n",
      "FastEstimator-Train: step: 2500; ce: 0.008084664; steps/sec: 70.73;\n",
      "FastEstimator-Train: step: 2600; ce: 0.047113482; steps/sec: 71.11;\n",
      "FastEstimator-Train: step: 2700; ce: 0.01939332; steps/sec: 73.15;\n",
      "FastEstimator-Train: step: 2800; ce: 0.0058479137; steps/sec: 70.8;\n",
      "FastEstimator-Train: step: 2900; ce: 0.0133756; steps/sec: 70.4;\n",
      "FastEstimator-Train: step: 3000; ce: 0.00542433; steps/sec: 66.53;\n",
      "FastEstimator-Train: step: 3100; ce: 0.0123183625; steps/sec: 66.0;\n",
      "FastEstimator-Train: step: 3200; ce: 0.0035992165; steps/sec: 67.31;\n",
      "FastEstimator-Train: step: 3300; ce: 0.029546442; steps/sec: 68.5;\n",
      "FastEstimator-Train: step: 3400; ce: 0.07383997; steps/sec: 71.68;\n",
      "FastEstimator-Train: step: 3500; ce: 0.0014300543; steps/sec: 71.02;\n",
      "FastEstimator-Train: step: 3600; ce: 0.00060256035; steps/sec: 69.73;\n",
      "FastEstimator-Train: step: 3700; ce: 0.034662727; steps/sec: 68.65;\n",
      "FastEstimator-Train: step: 3750; epoch: 2; epoch_time: 27.18 sec;\n",
      "Eval Progress: 1/312;\n",
      "Eval Progress: 104/312; steps/sec: 203.92;\n",
      "Eval Progress: 208/312; steps/sec: 202.22;\n",
      "Eval Progress: 312/312; steps/sec: 208.36;\n",
      "FastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpz8b4p4k4/model_best_accuracy.h5\n",
      "FastEstimator-Eval: step: 3750; epoch: 2; accuracy: 0.9896; ce: 0.03458583; max_accuracy: 0.9896; since_best_accuracy: 0;\n",
      "FastEstimator-Finish: step: 3750; model_lr: 0.001; total_time: 57.84 sec;\n"
     ]
    }
   ],
   "source": [
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='t01Inferencing'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferencing\n",
    "After training, we can do inferencing on new data with `Pipeline.transform` and `Netowork.transform`. Please checkout [Tutorial 8](./t08_mode.ipynb) for more details. \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth class is 7\n",
      "Predicted class is 7\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = eval_data[0]\n",
    "data = pipeline.transform(data, mode=\"eval\")\n",
    "data = network.transform(data, mode=\"eval\")\n",
    "\n",
    "print(\"Ground truth class is {}\".format(data[\"y\"][0]))\n",
    "print(\"Predicted class is {}\".format(np.argmax(data[\"y_pred\"])))\n",
    "img = fe.util.BatchDisplay(title=\"x\", image=data[\"x\"])\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='t01Apphub'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apphub Examples\n",
    "You can find some practical examples of the concepts described here in the following FastEstimator Apphubs:\n",
    "\n",
    "* [MNIST](../../apphub/image_classification/mnist/mnist.ipynb)\n",
    "* [DNN](../../apphub/tabular/dnn/dnn.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
