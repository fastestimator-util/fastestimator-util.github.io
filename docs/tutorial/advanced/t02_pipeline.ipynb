{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Tutorial 2: Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will discuss the following topics:\n",
    "\n",
    "* [Iterating Through Pipeline](#ta02itp)\n",
    "    * [Basic Concept](#ta02bc)\n",
    "    * [Example](#ta02example)\n",
    "* [Advanced Batching Control](#ta02batchop)\n",
    "    * [Dropping Last Batch](#ta02dlb)\n",
    "    * [Padding Batch Data](#ta02pbd)\n",
    "    * [Numpy Ops on Batches of Data](#ta02bn)\n",
    "    * [Filtering Data](#ta02filtering)\n",
    "* [Benchmark Pipeline Speed](#ta02bps)\n",
    "\n",
    "In the [Beginner Tutorial 4](../beginner/t04_pipeline.ipynb), we learned how to build a data pipeline that handles data loading and preprocessing tasks efficiently. Now that you have understood some basic operations in the `Pipeline`, we will demonstrate some advanced concepts and how to leverage them to create efficient `Pipelines` in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ta02itp'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating Through Pipeline\n",
    "\n",
    "In many deep learning tasks, the parameters for preprocessing tasks are precomputed by looping through the dataset. For example, in the `ImageNet` dataset, people usually use a precomputed global pixel average for each channel to normalize the images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ta02bc'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will see how to iterate through the pipeline in FastEstimator. First we will create a sample NumpyDataset from the data dictionary and load it into a `Pipeline`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from fastestimator.dataset.data import cifair10\n",
    "    \n",
    "# sample numpy array to later create datasets from them\n",
    "x_train, y_train = (np.random.sample((10, 2)), np.random.sample((10, 1)))\n",
    "train_data = {\"x\": x_train, \"y\": y_train}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastestimator as fe\n",
    "from fastestimator.dataset.numpy_dataset import NumpyDataset\n",
    "\n",
    "# create NumpyDataset from the sample data\n",
    "dataset_fe = NumpyDataset(train_data)\n",
    "\n",
    "pipeline_fe = fe.Pipeline(train_data=dataset_fe, batch_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the loader object for the `Pipeline`, then iterate through the loader with a for loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': tensor([[0.7792, 0.4546],\n",
      "        [0.6361, 0.7613],\n",
      "        [0.5676, 0.9048]], dtype=torch.float64), 'y': tensor([[0.9328],\n",
      "        [0.9089],\n",
      "        [0.1312]], dtype=torch.float64)}\n",
      "{'x': tensor([[0.0175, 0.2374],\n",
      "        [0.3992, 0.8328],\n",
      "        [0.7125, 0.0620]], dtype=torch.float64), 'y': tensor([[0.4130],\n",
      "        [0.9074],\n",
      "        [0.3998]], dtype=torch.float64)}\n",
      "{'x': tensor([[0.0584, 0.7026],\n",
      "        [0.9152, 0.5944],\n",
      "        [0.5536, 0.4152]], dtype=torch.float64), 'y': tensor([[0.0863],\n",
      "        [0.5301],\n",
      "        [0.6771]], dtype=torch.float64)}\n",
      "{'x': tensor([[0.4141, 0.0859]], dtype=torch.float64), 'y': tensor([[0.5408]], dtype=torch.float64)}\n"
     ]
    }
   ],
   "source": [
    "with pipeline_fe(mode=\"train\") as loader_fe:\n",
    "    for batch in loader_fe:\n",
    "        print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ta02example'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we have the ciFAIR-10 dataset and we want to find global average pixel value over three channels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.dataset.data import cifair10\n",
    "\n",
    "cifair_train, _ = cifair10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will take the `batch_size` 64 and load the data into `Pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_cifair = fe.Pipeline(train_data=cifair_train, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will iterate through batch data and compute the mean pixel values for all three channels of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pipeline_cifair(mode=\"train\", shuffle=False) as loader_fe:\n",
    "    mean_arr = np.zeros((3))\n",
    "    for i, batch in enumerate(loader_fe):\n",
    "        mean_arr = mean_arr + np.mean(batch[\"x\"].numpy(), axis=(0, 1, 2))\n",
    "    mean_arr = mean_arr / (i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean pixel value over the channels are:  [125.32287898 122.96682199 113.8856495 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean pixel value over the channels are: \", mean_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ta02batchop'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Batching Control\n",
    "\n",
    "Sometimes you may need advanced control over pipeline batching behavior, or even to run pipeline ops on an entire batch of data at once rather than on individual samples. Both of these use cases are enabled through the `Batch` Op. We'll start with looking at how you can customize batching behavior using the op:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ta02dlb'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Last Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the total number of dataset elements is not divisible by the `batch_size`, by default, the last batch will have less data than other batches.  To drop the last batch we can set `drop_last` to `True`. Therefore, if the last batch is incomplete it will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.op.numpyop import Batch\n",
    "\n",
    "pipeline_fe = fe.Pipeline(train_data=dataset_fe, ops=[Batch(batch_size=3, drop_last=True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `Batch` is an op, you can schedule it's behavior to change over different epochs (see [Advanced Tutorial 5](./t05_scheduler.ipynb)), as well as for specific modes or datasets (see [Advanced Tutorial 13](./t13_multi-dataset_training_evaluation.ipynb))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ta02pbd'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding Batch Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There might be scenario where the input tensors have different dimensions within a batch. For example, in Natural Language Processing, we have input strings with different lengths. For that we need to pad the data to the maximum length within the batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To further illustrate in code, we will take numpy array that contains different shapes of array elements and load it into the `Pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define numpy arrays with different shapes\n",
    "elem1 = np.array([4, 5])\n",
    "elem2 = np.array([1, 2, 6])\n",
    "elem3 = np.array([3])\n",
    "\n",
    "# create train dataset\n",
    "x_train = np.array([elem1, elem2, elem3], dtype=object)\n",
    "train_data = {\"x\": x_train}\n",
    "dataset_fe = NumpyDataset(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will set any `pad_value` that we want to append at the end of the tensor data. `pad_value` can be either `int` or `float`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_fe = fe.Pipeline(train_data=dataset_fe, ops=[Batch(batch_size=3, pad_value=0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's print the batch data after padding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': tensor([[4, 5, 0],\n",
      "        [1, 2, 6],\n",
      "        [3, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "with pipeline_fe(mode=\"train\", shuffle=False) as loader_fe:\n",
    "    for elem in loader_fe:\n",
    "        print(elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ta02bn'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy Ops on Batches of Data\n",
    "\n",
    "Normally `Pipeline` ops run on individual data instances before they are combined together into a batch. There might, however, be certain instances where you need to run an op on the entire batch of data at once. You could use a `TensorOp` in the `Network` to accomplish this, but it is also possible in the `Pipeline` by placing your `NumpyOp` after the `Batch` Op in the op list. This is generally less efficient than performing pre-processing on a per-instance level though, so we recommend only using the feature if you are certain that you need it. This process uses the forward_batch method of `NumpyOp`s, which has a default implementation that breaks the batch apart and runs the forward method on each individual instance before recombining the batch. A handful of Ops override this default behavior to take advantage of the full batch information. If you want to implement a custom op that leverages all of the available batch information, take a look at the `NumpyOp` base class implementation for more information. \n",
    "\n",
    "For now, let's consider a simple example using a `LambdaOp` which will subtract the batch-global mean from each sample in the batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([[-4., -3., -2.],\n",
       "         [-1.,  0.,  1.],\n",
       "         [ 2.,  3.,  4.]], dtype=torch.float64)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create train dataset\n",
    "train_data = {\"x\": np.array([[1.0, 2.0, 3.0], [4, 5, 6], [7, 8 ,9]])}\n",
    "dataset_fe = NumpyDataset(train_data)\n",
    "\n",
    "#Imports\n",
    "from fastestimator.op.numpyop import LambdaOp\n",
    "from fastestimator.backend import reduce_mean\n",
    "\n",
    "# Set up the pipeline\n",
    "pipeline_fe = fe.Pipeline(train_data=dataset_fe, \n",
    "                          ops=[Batch(batch_size=3),\n",
    "                               LambdaOp(inputs=\"x\", outputs=\"x\", fn=lambda x: x-reduce_mean(x))\n",
    "                              ])\n",
    "\n",
    "# Check the results\n",
    "pipeline_fe.get_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the batch mean (5) was successfully subtracted from each sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ta02filtering'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Data\n",
    "\n",
    "Suppose that you want more control over the composition of a particular batch of data. For example, you might have some bad data you want to exclude, or difficult samples that you want to save for later during training. While it would be more computationally efficient to modify your dataset to exclude undesirable samples, you can also apply a filter inside the `Pipeline` using the `RemoveIf` Op. This can be applied either before or after the `Batch` Op depending on your requirements. Let's take a look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch mean: 0.5012458402998198\n",
      "sample max: 0.959835264378248\n",
      "sample max: 0.9998099671968909\n",
      "sample max: 0.9553317550086283\n",
      "sample max: 0.9985566586444284\n",
      "---\n",
      "batch mean: 0.5615718094782679\n",
      "sample max: 0.9934757632773238\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "from fastestimator.backend import reduce_max\n",
    "from fastestimator.dataset import GeneratorDataset\n",
    "from fastestimator.op.numpyop import RemoveIf\n",
    "\n",
    "# Let's start with a dataset that generates random 5x5x3 'images'\n",
    "image_generator = ({'x':np.random.rand(5,5,3)} for _ in iter(int, 1))\n",
    "train_data = GeneratorDataset(samples_per_epoch=5, generator=image_generator)\n",
    "\n",
    "# Now let's remove individual images if they don't have at least 1 value greater than 0.9\n",
    "# Let's also remove batches of images if the mean of the batch is less than 0.6\n",
    "pipeline_fe = fe.Pipeline(train_data=train_data,\n",
    "                          ops=[RemoveIf(inputs='x', fn=lambda x: reduce_max(x) <= 0.9),\n",
    "                               Batch(batch_size=4),\n",
    "                               RemoveIf(inputs='x', fn=lambda x: reduce_mean(x) < 0.5)\n",
    "                              ])\n",
    "\n",
    "# Check the results\n",
    "batches = pipeline_fe.get_results(num_steps=2)\n",
    "for batch in batches:\n",
    "    print(f\"batch mean: {reduce_mean(batch['x'])}\")\n",
    "    for sample in batch['x']:\n",
    "        print(f\"sample max: {reduce_max(sample)}\")\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that since the dataset specified that it contained 5 samples (samples_per_epoch=5), there were still 5 samples in the output after filtering. The `RemoveIf` op defaults to filtering with replacement, meaning that discarded samples are replaced with other samples from the dataset. If you wish to discard without replacement (for example, in eval mode), you can set replacement=False. When replacement is True the system will still draw all of the available data once before repeating samples. See the `RemoveIf` docs for more detailed information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ta02bps'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Pipeline Speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often the case that the bottleneck of deep learning training is the data pipeline. As a result, the GPU may be underutilized. FastEstimator provides a method to check the speed of a `Pipeline` in order to help diagnose any potential problems. The way to benchmark `Pipeline` speed in FastEstimator is very simple: call `Pipeline.benchmark`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For illustration, we will create a `Pipeline` for the ciFAIR-10 dataset with list of Numpy operators that expand dimensions, apply `Minmax` and finally `Rotate` the input images: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.op.numpyop.univariate import Minmax, ExpandDims\n",
    "from fastestimator.op.numpyop.multivariate import Rotate\n",
    "\n",
    "pipeline = fe.Pipeline(train_data=cifair_train,\n",
    "                       ops=[Minmax(inputs=\"x\", outputs=\"x_out\"),\n",
    "                            Rotate(image_in=\"x_out\", image_out=\"x_out\", limit=180),\n",
    "                            ExpandDims(inputs=\"x_out\", outputs=\"x_out\", mode=\"train\")],\n",
    "                       batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's benchmark the pre-processing speed for this pipeline in training mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastEstimator-Benchmark: Dataset: , Step: 100, Epoch: 1, Steps/sec: 189.0564267416377\n",
      "FastEstimator-Benchmark: Dataset: , Step: 200, Epoch: 1, Steps/sec: 254.93623924284935\n",
      "FastEstimator-Benchmark: Dataset: , Step: 300, Epoch: 1, Steps/sec: 243.89103090633392\n",
      "FastEstimator-Benchmark: Dataset: , Step: 400, Epoch: 1, Steps/sec: 281.2587460839808\n",
      "FastEstimator-Benchmark: Dataset: , Step: 500, Epoch: 1, Steps/sec: 262.6512477872427\n",
      "FastEstimator-Benchmark: Dataset: , Step: 600, Epoch: 1, Steps/sec: 260.0439313025715\n",
      "FastEstimator-Benchmark: Dataset: , Step: 700, Epoch: 1, Steps/sec: 235.81544039811544\n",
      "FastEstimator-Benchmark: Dataset: , Step: 800, Epoch: 1, Steps/sec: 231.6578051316986\n",
      "FastEstimator-Benchmark: Dataset: , Step: 900, Epoch: 1, Steps/sec: 228.99282500888296\n",
      "FastEstimator-Benchmark: Dataset: , Step: 1000, Epoch: 1, Steps/sec: 221.91052155741298\n",
      "Breakdown of time taken by Pipeline Operations (mode:train epoch:1, ds_id:)\n",
      "Op         : Inputs : Outputs :  Time\n",
      "--------------------------------------\n",
      "Minmax     : x      : x_out   : 39.91%\n",
      "Rotate     : x_out  : x_out   : 49.76%\n",
      "ExpandDims : x_out  : x_out   : 10.33%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline.benchmark(mode=\"train\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
