{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Tutorial 17: Slicer\n",
    "\n",
    "## Overview\n",
    "In this tutorial, we will talk about the following topics:\n",
    "* [Slicer Overview](#ta17overview)\n",
    "* [Example Usecase 1: 3D to 2D](#ta17axis)\n",
    "* [Example Usecase 2: Sliding Windows](#ta17slide)\n",
    "* [Building Your Own Slicer](#ta17customize)\n",
    "\n",
    "This tutorial will demonstrate modifications to the [UNet3D Apphub](../../apphub/semantic_segmentation/unet3d_3plus/unet3d_3plus.ipynb), so you may want to look at that first to get more context for the problem setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ta17overview'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slicer Overview\n",
    "\n",
    "Suppose you have a single batch of data, but for whatever reason you can't / don't want to run the entire thing through your model at once. `Slicers` allow you to cut a batch of data apart and run it through your `Network` in chunks. `Slicers` then re-combine the output before passing it along to `Network` post-processing and `Trace` functions. \n",
    "\n",
    "In this tutorial we will take a look at this through the lens of an [electronic microscopy 3D cell dataset](https://leapmanlab.github.io/dense-cell/). This dataset contains only 2 images: one of size 800x800x50 and the other 800x800x24. These would be rather large images to feed through a network in a single step. \n",
    "\n",
    "In our [UNet3D Apphub](../../apphub/semantic_segmentation/unet3d_3plus/unet3d_3plus.ipynb) we took advantage of the FE dataset implementation to automatically convert the data volumes into around 75 training images and 25 validation images, each of size 256x256x24. That was fine for our 3D model, but what if you want to pass the data directly into a 2D network? Or what if you think side lengths of 256 are too boring and you want to try something else instead? Well, in that case you have come to the right place. Let's see how to make it happen..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ta17axis'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Example Usecase 1: 3D data into a 2D Network\n",
    "\n",
    "One reason that you might want to slice data apart is if you have 3D image volumes, but you want to inference them slice-by-slice using a 2D network architecture. This can be accommodated using an `AxisSlicer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### The Data\n",
    "\n",
    "First let's load up some 3D data and have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Samples: 75\n",
      "Eval Samples: 25\n",
      "Sample Shape: (256, 256, 24)\n"
     ]
    }
   ],
   "source": [
    "from fastestimator.dataset.data.em_3d import load_data\n",
    "\n",
    "train_data, eval_data = load_data()\n",
    "print(f\"Training Samples: {len(train_data)}\")\n",
    "print(f\"Eval Samples: {len(eval_data)}\")\n",
    "print(f\"Sample Shape: {train_data[0]['image'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll feed this data through a pretty straightforward pre-processing pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import fastestimator as fe\n",
    "from fastestimator.op.numpyop.meta import Sometimes\n",
    "from fastestimator.op.numpyop.multivariate import HorizontalFlip, VerticalFlip\n",
    "from fastestimator.op.numpyop.univariate import Minmax\n",
    "from fastestimator.op.numpyop.univariate.expand_dims import ExpandDims\n",
    "\n",
    "pipeline = fe.Pipeline(\n",
    "    train_data=train_data,\n",
    "    eval_data=eval_data,\n",
    "    batch_size=1,\n",
    "    ops=[\n",
    "        Sometimes(numpy_op=HorizontalFlip(image_in=\"image\", mask_in=\"label\", mode='train')),\n",
    "        Sometimes(numpy_op=VerticalFlip(image_in=\"image\", mask_in=\"label\", mode='train')),\n",
    "        Minmax(inputs=\"image\", outputs=\"image\"),\n",
    "        ExpandDims(inputs=\"image\", outputs=\"image\"),  # We'll add a channel dimension to the images\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Batch Shape: torch.Size([1, 256, 256, 24, 1])\n",
      "Label Batch Shape: torch.Size([1, 256, 256, 24, 6])\n"
     ]
    }
   ],
   "source": [
    "sample_batch = pipeline.get_results()\n",
    "print(f\"Image Batch Shape: {sample_batch['image'].shape}\")\n",
    "print(f\"Label Batch Shape: {sample_batch['label'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Network\n",
    "\n",
    "In our apphub we spent quite a lot of time defining a 3D model architecture that could handle this data. But what if you just want to use a basic 2D UNet Model? In that case you'll want to use an `AxisSlicer`. The `AxisSlicer` slices our input data along a given axis and then runs each slice through the network separately. The slices are then put back together again before being passed on to subsequent `Network` post-processing `Ops` or `Traces`. You'll also need to specify how to un-slice (re-combine) the data which is generated during the repeated forward passes over the `Network`. In this case we will use a `MeanUnslicer` in order to merge our loss terms together for log printing, and re-use our `AxisSlicer` to stack the network predictions back into a 3D volume:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M2 Max\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-27 17:45:47.879720: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-06-27 17:45:47.879753: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from fastestimator.architecture.tensorflow import UNet\n",
    "from fastestimator.op.tensorop.loss import CrossEntropy\n",
    "from fastestimator.op.tensorop.model import ModelOp, UpdateOp\n",
    "from fastestimator.slicer import AxisSlicer, MeanUnslicer\n",
    "\n",
    "model = fe.build(model_fn=lambda: UNet(input_size=(256, 256, 1), output_channel=6),\n",
    "                 optimizer_fn=lambda: tf.optimizers.legacy.Adam(learning_rate=0.0001),)\n",
    "network = fe.Network(\n",
    "    ops=[\n",
    "        ModelOp(inputs=\"image\", model=model, outputs=\"pred\"),\n",
    "        CrossEntropy(inputs=(\"pred\", \"label\"), outputs=\"loss\", form=\"binary\"),\n",
    "        UpdateOp(model=model, loss_name=\"loss\")\n",
    "    ], \n",
    "    slicers=[\n",
    "        AxisSlicer(slice=[\"image\", \"label\"], unslice=[\"pred\"], axis=3), \n",
    "        MeanUnslicer(unslice=\"loss\")\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Batch Shape: (1, 256, 256, 24, 1)\n",
      "Label Batch Shape: (1, 256, 256, 24, 6)\n",
      "Prediction Batch Shape: (1, 256, 256, 24, 6)\n",
      "Mean Loss Value: 1.1275629997253418\n"
     ]
    }
   ],
   "source": [
    "sample_prediction = network.transform(data=sample_batch, mode='eval')\n",
    "print(f\"Image Batch Shape: {sample_prediction['image'].shape}\")\n",
    "print(f\"Label Batch Shape: {sample_prediction['label'].shape}\")\n",
    "print(f\"Prediction Batch Shape: {sample_prediction['pred'].shape}\")\n",
    "print(f\"Mean Loss Value: {sample_prediction['loss']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's add in a metric trace and see the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fastestimator.trace.metric import Dice\n",
    "\n",
    "channel_mapping={0: 'Cell', 1: 'Mitochondria', 2: 'AlphaGranule',\n",
    "                 3: 'CanalicularVessel', 4: 'GranuleBody', 5: 'GranuleCore'}\n",
    "\n",
    "traces=[Dice(true_key=\"label\", pred_key=\"pred\", channel_mapping=channel_mapping)]\n",
    "\n",
    "estimator = fe.Estimator(\n",
    "    pipeline=pipeline,\n",
    "    network=network,\n",
    "    traces=traces,\n",
    "    log_steps=75,\n",
    "    eval_log_steps=[-1],\n",
    "    epochs=10)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ______           __  ______     __  _                 __            \n",
      "   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n",
      "  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n",
      " / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n",
      "/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n",
      "                                                                        \n",
      "\n",
      "\u001b[93mFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\u001b[00m\n",
      "FastEstimator-Start: step: 1; logging_interval: 75; num_device: 1;\n",
      "FastEstimator-Train: step: 1; loss: 0.35076174;\n",
      "FastEstimator-Train: step: 75; loss: 0.06861534; steps/sec: 0.45;\n",
      "FastEstimator-Train: step: 75; epoch: 1; epoch_time(sec): 173.2;\n",
      "FastEstimator-Eval: step: 75; epoch: 1; Dice: 0.1650141; Dice_AlphaGranule: 0.0; Dice_CanalicularVessel: 0.0; Dice_Cell: 0.56379896; Dice_GranuleBody: 0.0; Dice_GranuleCore: 0.41382408; Dice_Mitochondria: 0.012461557; loss: 0.22645305;\n",
      "FastEstimator-Train: step: 150; loss: 0.036027152; steps/sec: 0.41;\n",
      "FastEstimator-Train: step: 150; epoch: 2; epoch_time(sec): 182.15;\n",
      "FastEstimator-Eval: step: 150; epoch: 2; Dice: 0.28468058; Dice_AlphaGranule: 0.22724058; Dice_CanalicularVessel: 0.0; Dice_Cell: 0.84920985; Dice_GranuleBody: 0.0; Dice_GranuleCore: 0.59404886; Dice_Mitochondria: 0.037584234; loss: 0.11351043;\n",
      "FastEstimator-Train: step: 225; loss: 0.019175015; steps/sec: 0.43;\n",
      "FastEstimator-Train: step: 225; epoch: 3; epoch_time(sec): 173.2;\n",
      "FastEstimator-Eval: step: 225; epoch: 3; Dice: 0.27223733; Dice_AlphaGranule: 0.14862165; Dice_CanalicularVessel: 0.0; Dice_Cell: 0.87892294; Dice_GranuleBody: 0.0; Dice_GranuleCore: 0.57796216; Dice_Mitochondria: 0.027917214; loss: 0.10777818;\n",
      "FastEstimator-Train: step: 300; loss: 0.038775463; steps/sec: 0.43;\n",
      "FastEstimator-Train: step: 300; epoch: 4; epoch_time(sec): 175.43;\n",
      "FastEstimator-Eval: step: 300; epoch: 4; Dice: 0.26352742; Dice_AlphaGranule: 0.06612307; Dice_CanalicularVessel: 0.0; Dice_Cell: 0.85590476; Dice_GranuleBody: 0.0; Dice_GranuleCore: 0.6462211; Dice_Mitochondria: 0.012915753; loss: 0.15005614;\n",
      "FastEstimator-Train: step: 375; loss: 0.041875094; steps/sec: 0.42;\n",
      "FastEstimator-Train: step: 375; epoch: 5; epoch_time(sec): 179.78;\n",
      "FastEstimator-Eval: step: 375; epoch: 5; Dice: 0.25157312; Dice_AlphaGranule: 0.09126292; Dice_CanalicularVessel: 0.0; Dice_Cell: 0.8459115; Dice_GranuleBody: 0.0; Dice_GranuleCore: 0.5518494; Dice_Mitochondria: 0.020414837; loss: 0.14314592;\n",
      "FastEstimator-Train: step: 450; loss: 0.036384713; steps/sec: 0.42;\n",
      "FastEstimator-Train: step: 450; epoch: 6; epoch_time(sec): 179.49;\n",
      "FastEstimator-Eval: step: 450; epoch: 6; Dice: 0.24621534; Dice_AlphaGranule: 0.07902191; Dice_CanalicularVessel: 0.0016777955; Dice_Cell: 0.81178457; Dice_GranuleBody: 0.0; Dice_GranuleCore: 0.5809567; Dice_Mitochondria: 0.0038511453; loss: 0.19332533;\n",
      "FastEstimator-Train: step: 525; loss: 0.032831687; steps/sec: 0.42;\n",
      "FastEstimator-Train: step: 525; epoch: 7; epoch_time(sec): 177.97;\n",
      "FastEstimator-Eval: step: 525; epoch: 7; Dice: 0.36281154; Dice_AlphaGranule: 0.3231712; Dice_CanalicularVessel: 0.15168218; Dice_Cell: 0.92331016; Dice_GranuleBody: 0.010409508; Dice_GranuleCore: 0.75507385; Dice_Mitochondria: 0.013222287; loss: 0.099501915;\n",
      "FastEstimator-Train: step: 600; loss: 0.039932176; steps/sec: 0.42;\n",
      "FastEstimator-Train: step: 600; epoch: 8; epoch_time(sec): 178.89;\n",
      "FastEstimator-Eval: step: 600; epoch: 8; Dice: 0.40107933; Dice_AlphaGranule: 0.43582454; Dice_CanalicularVessel: 0.15312457; Dice_Cell: 0.9320569; Dice_GranuleBody: 0.11979302; Dice_GranuleCore: 0.76528555; Dice_Mitochondria: 0.0003915782; loss: 0.07295823;\n",
      "FastEstimator-Train: step: 675; loss: 0.025439756; steps/sec: 0.42;\n",
      "FastEstimator-Train: step: 675; epoch: 9; epoch_time(sec): 180.04;\n",
      "FastEstimator-Eval: step: 675; epoch: 9; Dice: 0.39481708; Dice_AlphaGranule: 0.3910953; Dice_CanalicularVessel: 0.14935504; Dice_Cell: 0.930211; Dice_GranuleBody: 0.118480444; Dice_GranuleCore: 0.7636419; Dice_Mitochondria: 0.016118763; loss: 0.094853036;\n",
      "FastEstimator-Train: step: 750; loss: 0.016932765; steps/sec: 0.42;\n",
      "FastEstimator-Train: step: 750; epoch: 10; epoch_time(sec): 180.29;\n",
      "FastEstimator-Eval: step: 750; epoch: 10; Dice: 0.3541808; Dice_AlphaGranule: 0.17112847; Dice_CanalicularVessel: 0.16391844; Dice_Cell: 0.8922416; Dice_GranuleBody: 0.14522946; Dice_GranuleCore: 0.7323538; Dice_Mitochondria: 0.020212961; loss: 0.15120216;\n",
      "FastEstimator-Finish: step: 750; model_lr: 1e-04; total_time(sec): 2069.42;\n"
     ]
    }
   ],
   "source": [
    "# estimator.fit()  # Uncomment this if you want to re-run the training yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that your steps/sec as reported by logging will be much lower when using slicers than you might normally expect, since each 'step' is now actually 24 mini-steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ta17slide'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usecase 2: Using a sliding window to work around GPU memory limits\n",
    "\n",
    "Let's imagine a world where you wanted to work with this cell data, but in which FE had not implemented it's convenient dataset tiling feature for you. Instead you are confronted with some gigantic images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Samples: 1\n",
      "Eval Samples: 1\n",
      "Train Image Shape: (800, 800, 50)\n",
      "Train Label Shape: (800, 800, 50, 6)\n",
      "Eval Sample Shape: (800, 800, 24)\n",
      "Eval Label Shape: (800, 800, 24, 6)\n"
     ]
    }
   ],
   "source": [
    "from fastestimator.dataset.data.em_3d import load_data\n",
    "\n",
    "train_data, eval_data = load_data(tile=False)\n",
    "print(f\"Training Samples: {len(train_data)}\")\n",
    "print(f\"Eval Samples: {len(eval_data)}\")\n",
    "print(f\"Train Image Shape: {train_data[0]['image'].shape}\")\n",
    "print(f\"Train Label Shape: {train_data[0]['label'].shape}\")\n",
    "print(f\"Eval Sample Shape: {eval_data[0]['image'].shape}\")\n",
    "print(f\"Eval Label Shape: {eval_data[0]['label'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the same pipeline processing from our first usecase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import fastestimator as fe\n",
    "from fastestimator.op.numpyop.meta import Sometimes\n",
    "from fastestimator.op.numpyop.multivariate import HorizontalFlip, VerticalFlip\n",
    "from fastestimator.op.numpyop.univariate import Minmax\n",
    "from fastestimator.op.numpyop.univariate.expand_dims import ExpandDims\n",
    "\n",
    "pipeline = fe.Pipeline(\n",
    "    train_data=train_data,\n",
    "    eval_data=eval_data,\n",
    "    batch_size=1,\n",
    "    ops=[\n",
    "        Sometimes(numpy_op=HorizontalFlip(image_in=\"image\", mask_in=\"label\", mode='train')),\n",
    "        Sometimes(numpy_op=VerticalFlip(image_in=\"image\", mask_in=\"label\", mode='train')),\n",
    "        Minmax(inputs=\"image\", outputs=\"image\"),\n",
    "        ExpandDims(inputs=\"image\", outputs=\"image\"),  # We'll add a channel dimension to the images\n",
    "    ], \n",
    "    num_process=0)  # Since we only have 1 image to work with we'll turn off multi-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Batch Shape: torch.Size([1, 800, 800, 50, 1])\n",
      "Label Batch Shape: torch.Size([1, 800, 800, 50, 6])\n"
     ]
    }
   ],
   "source": [
    "sample_batch = pipeline.get_results()\n",
    "print(f\"Image Batch Shape: {sample_batch['image'].shape}\")\n",
    "print(f\"Label Batch Shape: {sample_batch['label'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Network\n",
    "\n",
    "Rather than cutting the data beforehand, we can use a `SlidingSlicer` to move over it in a sliding-window fashion. Each chunk of data is run through the network separately and then re-combined before being passed on to subsequent `Network` post-processing `Ops` or `Traces`. You'll also need to specify how to un-slice (re-combine) the data which is generated during the repeated forward passes over the `Network`. In this case we will use a `MeanUnslicer` in order to merge our loss terms together for log printing, and re-use our `SlidingSlicer` to paste the network predictions back into a 3D volume.\n",
    "\n",
    "Let's cut our image into windows of size (256, 256, 1) and then run them through a 2D network. In this case we don't want to cut along the batch or channel dimensions, so we will set their values to -1 in the `SlidingSlicer` window_size argument. Note that 256 does not evenly divide into 800. SlidingSlicer gives you several ways to handle this. By default it will simply drop any leftover data along each window axis. You can instead keep partial slices by switching the 'pad_mode' to 'partial', or using padding to fill out the final slices by switching 'pad_mode' to 'constant'. You could also consider customizing the 'strides' such that each window partially overlaps with the previous in such a way that your overall output properly tiles. In our example, you could stride by 136 to achieve this (136*4+256=800). When overlapping strides are re-combined later, any values where there is overlap will be averaged together. If you prefer a simple sum you can change this by modifying the 'unslice_mode' option.\n",
    "\n",
    "Let's use the stride method here for the sake of example. Since we are feeding our data into a 2D network, we'll also use the 'squeeze_window' ability of our SlidingSlicer to automatically remove our z axis during forward passes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from fastestimator.architecture.tensorflow import UNet\n",
    "from fastestimator.op.tensorop.loss import CrossEntropy\n",
    "from fastestimator.op.tensorop.model import ModelOp, UpdateOp\n",
    "from fastestimator.slicer import SlidingSlicer, MeanUnslicer\n",
    "\n",
    "model = fe.build(model_fn=lambda: UNet(input_size=(256, 256, 1), output_channel=6),\n",
    "                 optimizer_fn=lambda: tf.optimizers.legacy.Adam(learning_rate=0.0001),)\n",
    "network = fe.Network(\n",
    "    ops=[\n",
    "        ModelOp(inputs=\"image\", model=model, outputs=\"pred\"),\n",
    "        CrossEntropy(inputs=(\"pred\", \"label\"), outputs=\"loss\", form=\"binary\"),\n",
    "        UpdateOp(model=model, loss_name=\"loss\")\n",
    "    ], \n",
    "    slicers=[\n",
    "        SlidingSlicer(slice=[\"image\", \"label\"], \n",
    "                      unslice=[\"pred\"], \n",
    "                      window_size=(-1, 256, 256, 1, -1), \n",
    "                      strides=(0, 136, 136, 1, 0),\n",
    "                      squeeze_window=True,\n",
    "                     ),\n",
    "        MeanUnslicer(unslice=\"loss\")\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a sample prediction after running through the network. This is going to take a while, since a single step is now responsible for processing 5x5x50=1250 mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Batch Shape: (1, 800, 800, 50, 1)\n",
      "Label Batch Shape: (1, 800, 800, 50, 6)\n",
      "Prediction Batch Shape: (1, 800, 800, 50, 6)\n",
      "Mean Loss Value: 0.9557278156280518\n"
     ]
    }
   ],
   "source": [
    "# sample_prediction = network.transform(data=sample_batch, mode='eval')  # Uncomment this if you want to inspect the shapes yourself\n",
    "# print(f\"Image Batch Shape: {sample_prediction['image'].shape}\")\n",
    "# print(f\"Label Batch Shape: {sample_prediction['label'].shape}\")\n",
    "# print(f\"Prediction Batch Shape: {sample_prediction['pred'].shape}\")\n",
    "# print(f\"Mean Loss Value: {sample_prediction['loss']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's add in a metric trace and see the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fastestimator.trace.metric import Dice\n",
    "\n",
    "channel_mapping={0: 'Cell', 1: 'Mitochondria', 2: 'AlphaGranule',\n",
    "                 3: 'CanalicularVessel', 4: 'GranuleBody', 5: 'GranuleCore'}\n",
    "\n",
    "traces=[Dice(true_key=\"label\", pred_key=\"pred\", channel_mapping=channel_mapping)]\n",
    "\n",
    "estimator = fe.Estimator(\n",
    "    pipeline=pipeline,\n",
    "    network=network,\n",
    "    traces=traces,\n",
    "    log_steps=1,\n",
    "    eval_log_steps=[-1],\n",
    "    epochs=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ______           __  ______     __  _                 __            \n",
      "   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n",
      "  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n",
      " / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n",
      "/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n",
      "                                                                        \n",
      "\n",
      "\u001b[93mFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\u001b[00m\n",
      "FastEstimator-Start: step: 1; logging_interval: 1; num_device: 1;\n",
      "FastEstimator-Train: step: 1; loss: 0.09241561;\n",
      "FastEstimator-Train: step: 1; epoch: 1; epoch_time(sec): 187.88;\n",
      "FastEstimator-Eval: step: 1; epoch: 1; Dice: 0.19790895; Dice_AlphaGranule: 0.0; Dice_CanalicularVessel: 0.0; Dice_Cell: 0.76499873; Dice_GranuleBody: 0.0; Dice_GranuleCore: 0.42245498; Dice_Mitochondria: 0.0; loss: 0.17678283;\n",
      "FastEstimator-Train: step: 2; loss: 0.05950647; steps/sec: 0.0;\n",
      "FastEstimator-Train: step: 2; epoch: 2; epoch_time(sec): 207.26;\n",
      "FastEstimator-Eval: step: 2; epoch: 2; Dice: 0.31357098; Dice_AlphaGranule: 0.27818844; Dice_CanalicularVessel: 0.0; Dice_Cell: 0.91047233; Dice_GranuleBody: 0.0; Dice_GranuleCore: 0.68593067; Dice_Mitochondria: 0.006834461; loss: 0.09680609;\n",
      "FastEstimator-Train: step: 3; loss: 0.047980543; steps/sec: 0.0;\n",
      "FastEstimator-Train: step: 3; epoch: 3; epoch_time(sec): 225.46;\n",
      "FastEstimator-Eval: step: 3; epoch: 3; Dice: 0.2589626; Dice_AlphaGranule: 0.00012370278; Dice_CanalicularVessel: 0.0; Dice_Cell: 0.87941015; Dice_GranuleBody: 0.0; Dice_GranuleCore: 0.6742417; Dice_Mitochondria: 0.0; loss: 0.13281296;\n",
      "FastEstimator-Train: step: 4; loss: 0.03970469; steps/sec: 0.0;\n",
      "FastEstimator-Train: step: 4; epoch: 4; epoch_time(sec): 303.56;\n",
      "FastEstimator-Eval: step: 4; epoch: 4; Dice: 0.2699795; Dice_AlphaGranule: 0.0039450238; Dice_CanalicularVessel: 0.0; Dice_Cell: 0.87593764; Dice_GranuleBody: 0.0; Dice_GranuleCore: 0.738118; Dice_Mitochondria: 0.0018762958; loss: 0.15984353;\n",
      "FastEstimator-Train: step: 5; loss: 0.035081092; steps/sec: 0.0;\n",
      "FastEstimator-Train: step: 5; epoch: 5; epoch_time(sec): 343.57;\n",
      "FastEstimator-Eval: step: 5; epoch: 5; Dice: 0.28861472; Dice_AlphaGranule: 0.10057701; Dice_CanalicularVessel: 0.0; Dice_Cell: 0.878825; Dice_GranuleBody: 0.0; Dice_GranuleCore: 0.73525584; Dice_Mitochondria: 0.017030539; loss: 0.15729764;\n",
      "FastEstimator-Train: step: 6; loss: 0.030691482; steps/sec: 0.0;\n",
      "FastEstimator-Train: step: 6; epoch: 6; epoch_time(sec): 373.16;\n",
      "FastEstimator-Eval: step: 6; epoch: 6; Dice: 0.2974657; Dice_AlphaGranule: 0.10909026; Dice_CanalicularVessel: 0.0; Dice_Cell: 0.91101515; Dice_GranuleBody: 0.0; Dice_GranuleCore: 0.74959046; Dice_Mitochondria: 0.0150984535; loss: 0.134259;\n",
      "FastEstimator-Train: step: 7; loss: 0.040506963; steps/sec: 0.0;\n",
      "FastEstimator-Train: step: 7; epoch: 7; epoch_time(sec): 384.6;\n",
      "FastEstimator-Eval: step: 7; epoch: 7; Dice: 0.3132342; Dice_AlphaGranule: 0.36752912; Dice_CanalicularVessel: 0.10559372; Dice_Cell: 0.8301301; Dice_GranuleBody: 0.0; Dice_GranuleCore: 0.55129784; Dice_Mitochondria: 0.02485453; loss: 0.1310871;\n",
      "FastEstimator-Train: step: 8; loss: 0.03810389; steps/sec: 0.0;\n",
      "FastEstimator-Train: step: 8; epoch: 8; epoch_time(sec): 488.05;\n",
      "FastEstimator-Eval: step: 8; epoch: 8; Dice: 0.3384758; Dice_AlphaGranule: 0.10329177; Dice_CanalicularVessel: 0.3118691; Dice_Cell: 0.8723342; Dice_GranuleBody: 0.0; Dice_GranuleCore: 0.69752693; Dice_Mitochondria: 0.045832805; loss: 0.1552314;\n",
      "FastEstimator-Train: step: 9; loss: 0.030884113; steps/sec: 0.0;\n",
      "FastEstimator-Train: step: 9; epoch: 9; epoch_time(sec): 550.86;\n",
      "FastEstimator-Eval: step: 9; epoch: 9; Dice: 0.34353375; Dice_AlphaGranule: 0.12995201; Dice_CanalicularVessel: 0.033380825; Dice_Cell: 0.87134874; Dice_GranuleBody: 0.25112656; Dice_GranuleCore: 0.74927706; Dice_Mitochondria: 0.026117384; loss: 0.15764695;\n",
      "FastEstimator-Train: step: 10; loss: 0.027712101; steps/sec: 0.0;\n",
      "FastEstimator-Train: step: 10; epoch: 10; epoch_time(sec): 572.82;\n",
      "FastEstimator-Eval: step: 10; epoch: 10; Dice: 0.4659363; Dice_AlphaGranule: 0.08518501; Dice_CanalicularVessel: 0.6543881; Dice_Cell: 0.8772722; Dice_GranuleBody: 0.37757555; Dice_GranuleCore: 0.7437467; Dice_Mitochondria: 0.057450105; loss: 0.14906605;\n",
      "FastEstimator-Finish: step: 10; model1_lr: 1e-04; total_time(sec): 5205.04;\n"
     ]
    }
   ],
   "source": [
    "# estimator.fit()  # Uncomment this if you want to re-run the training yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that your steps/sec as reported by logging will be much lower when using slicers than you might normally expect, since each 'step' is now actually 1250 mini-steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ta17customize'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Your Own Slicer\n",
    "\n",
    "If the existing Slicers don't meet your needs, you can always customize your own. To do so, simply inherit the base class and implement one or both of the abstract methods defined there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.slicer.slicer import Slicer\n",
    "\n",
    "class DoubleBatchSlicer(Slicer):\n",
    "    def _slice_batch(self, batch):\n",
    "        # Implement this method if you want your slicer to be able to cut keys apart\n",
    "        # In this toy example we just duplicate the batch tensor twice\n",
    "        return [batch, batch]\n",
    "    \n",
    "    def _unslice_batch(self, slices, key):\n",
    "        # Implement this method if you want your slicer to be able to put keys back together\n",
    "        # In our toy example we just ignore the second mini-batch that we created\n",
    "        return slices[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a slightly more practical example, suppose that when recording loss values while using a slicer, you want to get the maximum loss rather than the mean. You could implement a MaxUnslicer as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxUnslicer(Slicer):\n",
    "    def __init__(self, unslice, mode=None, ds_id=None):\n",
    "        super().__init__(slice=None, unslice=unslice, mode=mode, ds_id=ds_id)\n",
    "    def _unslice_batch(self, slices, key):\n",
    "        maks = slices[0]\n",
    "        for minibatch in slices[1:]:\n",
    "            maks = max(maks, minibatch)\n",
    "        return maks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test out our new creation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss values: [[0.7506162 ]\n",
      " [0.07465518]\n",
      " [0.4861052 ]\n",
      " [0.9194704 ]\n",
      " [0.28890288]]\n",
      "number of slices: 5\n",
      "max loss: [0.9194704]\n"
     ]
    }
   ],
   "source": [
    "from fastestimator.slicer.slicer import forward_slicers, reverse_slicers\n",
    "\n",
    "loss = tf.random.uniform((5,1))\n",
    "print(f\"loss values: {loss}\")\n",
    "batch = {\"loss\": loss}\n",
    "\n",
    "axis_slicer = AxisSlicer(slice=\"loss\", unslice=None, axis=0)\n",
    "max_unslicer = MaxUnslicer(unslice=\"loss\")\n",
    "\n",
    "mini_batches = forward_slicers(slicers=[axis_slicer, max_unslicer], data=batch)\n",
    "print(f\"number of slices: {len(mini_batches)}\")\n",
    "output = reverse_slicers(slicers=[axis_slicer, max_unslicer], data=mini_batches, original_data=batch)\n",
    "\n",
    "print(f\"max loss: {output['loss']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
